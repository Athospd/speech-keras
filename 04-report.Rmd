---
title: "Simple audio classification with Keras"
output: html_document
author: "Daniel Falbel"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

In this tutorial we will build a deep learning model to classify different words. We will use [`tfdatasets`](https://tensorflow.rstudio.com/tools/tfdatasets/articles/introduction.html) to handle data IO and pre-processing and [Keras](https://keras.rstudio.com) to build and train the model.

We will use the [Speech Commands dataset](https://storage.cloud.google.com/download.tensorflow.org/data/speech_commands_v0.01.tar.gz) which consists of 65.000 one-second audio files of people saying 30 different words. Each file contains a single spoken English word. The dataset was released by Google under CC License.

Our model was first implemented in [*Simple Audio Recognition* at the TensorFlow documentation](https://www.tensorflow.org/tutorials/audio_recognition#top_of_page) which in turn was inpired by [*Convolutional Neural Networks for Small-footprint Keyword Spotting*](http://www.isca-speech.org/archive/interspeech_2015/papers/i15_1478.pdf). There are other approaches like [recurent neural networks](https://svds.com/tensorflow-rnn-tutorial/), [dilated (atrous) convolutions](https://deepmind.com/blog/wavenet-generative-model-raw-audio/) or [Learning from Between-class Examples for Deep Sound Recognition](https://arxiv.org/abs/1711.10282).

The model we will implement is not the state of the art for audio recognition systems, which are much more complex, but is relatively simple and fast to train.

## Audio representation

Many deep learning models are end-to-end, ie. we let the model learn useful representations directly from the raw data but, audio data grows very fast - 16,000 samples per seconds with a very rich structure at many time-scales. In order to avoid raw wave sound data, researchers usualy use some kind of feature engeneering.

Every sound wave can be represented by it's spectrum, and digitaly it can be computed using the [Fast Fourier Transform (FFT)](https://en.wikipedia.org/wiki/Fast_Fourier_transform).

![By Phonical - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=64473578](https://upload.wikimedia.org/wikipedia/commons/6/61/FFT-Time-Frequency-View.png)

A common way to represent audio data is break it in small chunks, which usualy overlap. For each chunk we use the FFT to calculate the magnitude of the frequency spectrum. The spectrums are then combined side by side to form what we call a [spectrogram](https://en.wikipedia.org/wiki/Spectrogram).

![By Aquegg - Own work, Public Domain, https://commons.wikimedia.org/w/index.php?curid=5544473](https://upload.wikimedia.org/wikipedia/commons/c/c5/Spectrogram-19thC.png)

After this procedure we have an image for each audio sample and we can use convolutional networks that are usual in image recognition models.






