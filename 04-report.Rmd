---
title: "Simple audio classification with Keras"
output: html_document
author: "Daniel Falbel"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```

## Introduction

In this tutorial we will build a deep learning model to classify words. We will use [`tfdatasets`](https://tensorflow.rstudio.com/tools/tfdatasets/articles/introduction.html) to handle data IO and pre-processing, and [Keras](https://keras.rstudio.com) to build and train the model.

We will use the [Speech Commands dataset](https://storage.cloud.google.com/download.tensorflow.org/data/speech_commands_v0.01.tar.gz) which consists of 65.000 one-second audio files of people saying 30 different words. Each file contains a single spoken English word. The dataset was released by Google under CC License (which one???).

Our model was first implemented in [*Simple Audio Recognition* at the TensorFlow documentation](https://www.tensorflow.org/tutorials/audio_recognition#top_of_page) which in turn was inpired by [*Convolutional Neural Networks for Small-footprint Keyword Spotting*](http://www.isca-speech.org/archive/interspeech_2015/papers/i15_1478.pdf). There are other approaches, like [recurent neural networks](https://svds.com/tensorflow-rnn-tutorial/), [dilated (atrous) convolutions](https://deepmind.com/blog/wavenet-generative-model-raw-audio/) or [Learning from Between-class Examples for Deep Sound Recognition](https://arxiv.org/abs/1711.10282).

The model we implement here is not the state of the art for audio recognition systems, which are way more complex, but is relatively simple and fast to train.

## Audio representation

Many deep learning models are end-to-end, i.e. we let the model learn useful representations directly from the raw data but, audio data grows very fast - 16,000 samples per second with a very rich structure at many time-scales. In order to avoid raw wave sound data, researchers usually use some kind of feature engineering.

Every sound wave can be represented by it's spectrum, and digitally it can be computed using the [Fast Fourier Transform (FFT)](https://en.wikipedia.org/wiki/Fast_Fourier_transform).

![By Phonical - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=64473578](https://upload.wikimedia.org/wikipedia/commons/6/61/FFT-Time-Frequency-View.png)

A common way to represent audio data is to break it into small chunks, which usualy overlap. For each chunk we use the FFT to calculate the magnitude of the frequency spectrum. The spectrums are then combined, side by side, to form what we call a [spectrogram](https://en.wikipedia.org/wiki/Spectrogram). 

It's also common for speech recognition systems to further transform the spectrum and compute the [Mel-Frequency Cepstral Coefficients](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum). This transformation takes into account that the human ear can't discern the difference between two closely spaced frequencies and smartly creates bins in the frequency axis. A great tutorial on MFCC's can be found [here](http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/).

![By Aquegg - Own work, Public Domain, https://commons.wikimedia.org/w/index.php?curid=5544473](https://upload.wikimedia.org/wikipedia/commons/c/c5/Spectrogram-19thC.png)

After this procedure, we have an image for each audio sample and we can use convolutional neural networks that are usual in image recognition models. 

## Downloading

First, let's download data to a directory in our project. You can whether download from [this link](http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz) (~1GB) or from R with:

```{r}
dir.create("data")

download.file(
  url = "http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz", 
  destfile = "data/speech_commands_v0.01.tar.gz"
)

untar("data/speech_commands_v0.01.tar.gz", exdir = "data/speech_commands_v0.01")
```

Inside the `data` directory we will have a folder called `speech_commands_v0.01`. The WAV audio files inside this directory are organised in sub-folders with the label names. For example, all one-second audio files of people speaking the word "bed" are inside the `bed` directory. There are 30 of them and a special one called `_background_noise_` which contains background noises that could be mixed in to simulate background noise.

## Importing

In this step we will list all audio .wav files into a `tibble` with 3 columns: 

* `fname`: the file name;
* `class`: the label for each audio file;
* `class_id`: a unique integer number starting from zero for each class - used to one-hot encode the classes.

This will be useful to the next step when we will create a generator using the `tfdatasets` package.

```{r}
library(stringr)
library(dplyr)

files <- fs::dir_ls(
  path = "data/speech_commands_v0.01/", 
  recursive = TRUE, 
  glob = "*.wav"
)

files <- files[!str_detect(files, "background_noise")]

df <- data_frame(
  fname = files, 
  class = fname %>% str_extract("1/.*/") %>% 
    str_replace_all("1/", "") %>%
    str_replace_all("/", ""),
  class_id = class %>% as.factor() %>% as.integer() - 1L
)
```

## Generator

We will now create our `Dataset`, which in the context of `tfdatasets`, adds operations to the TensorFlow graph in order to read and pre-process data. Since they are TensorFlow ops, they are executed in C++ and in parallel with model training.

The generator we will create will be responsible for reading the audio files from disk, creating the spectrogram for each one and batching the outputs.

Let's start defining creating the dataset from slices of the `data.frame` with audio file names and classes we just created.

```{r}
library(tfdatasets)
ds <- tensor_slices_dataset(df) 
```

Now, let's define the parameters for the spectrogram creation. We need to define the `frame_lenght` which is the size of each chunk we will break the audio wave; the `frame_step`: the number of samples to step between each chunk; `fft_lenght`: the size of the FFT to apply, an usual default value for the `fft_lenght` is the smallest power of 2 enclosing frame_length.

```{r}
frame_length <- 480L
frame_step <- 160L
fft_length <- as.integer(2^(as.integer(log(frame_length, 2)) + 1L))
```

We will now use `dataset_map` which allows us to specify a pre-processing function for each observation (line) of our dataset. It's in this step that we read the raw audio file from disk and create it's spectrogram and the one-hot encoded response vector.

```{r}
# shortcuts to used TensorFlow modules.
audio_ops <- tf$contrib$framework$python$ops$audio_ops
signal <- tf$contrib$signal 

ds <- ds %>%
  dataset_map(function(obs) {
    
    # a good way to debug when building tfdatsets pipelines is to use a print
    # statement like this:
    # print(str(obs))
    
    # decoding wav files
    audio_binary <- tf$read_file(tf$reshape(obs$fname, shape = list()))
    wav <- audio_ops$decode_wav(audio_binary, desired_channels = 1)
    
    # create the spectrogram
    spectrogram <- signal$stft(
      wav$audio[,1], 
      frame_length = frame_length, 
      frame_step = frame_step, 
      pad_end = TRUE, 
      fft_length = fft_length
    )
    
    # getting magnitudes squared
    spectrogram <- tf$real(spectrogram * tf$conj(spectrogram))
    # normalization
    spectrogram <- tf$log(tf$abs(spectrogram) + 0.01)
    
    # adds a dummy dimension since imgs are considered a 3d array in keras.
    spectrogram <- tf$expand_dims(spectrogram, -1L)
    
    # transform the class_id into a one-hot encoded vector
    response <- tf$one_hot(obs$class_id, 29L)
    
    list(spectrogram, response)
  }) 
```

Now, we will specify how we want batch observations from the dataset. We used `dataset_shuffle` since we want to shuffle observations from the dataset, otherwise it would follow the order of the `df` object. Then we used `dataset_repeat` in order to tell TensorFlow that we want to keep taking observations from the dataset even if all observations were already used. And most importantly here, we use `dataset_padded_batch` to specify that we want batches of size 32, but they should be padded, ie. if some observation has a different size we pad it with zeroes. The padded shapes is passed to `dataset_padded_batch` via the `padded_shapes` argument and we use `NULL` to state that this dimension doesn't need to be padded.

```{r}
ds <- ds %>% 
  dataset_shuffle(buffer_size = 100) %>%
  dataset_repeat() %>%
  dataset_padded_batch(
    batch_size = 32, 
    padded_shapes = list(
      shape(16000/frame_step, fft_length/2 + 1, NULL), 
      shape(NULL)
    )
  )
```

This is our Dataset specification, but we would need to rewrite all the code for the validation data, so it's a good practice to wrap this into a function of the data and other important parameters like the `frame_lenght`, `frame_step` and etc. Below, we will define a function called `data_geneartor` that will create the generator depending on those inputs.

```{r}
data_generator <- function(df, batch_size, shuffle = TRUE, 
                           frame_length = 480L, frame_step = 160L) {
  
  ds <- tensor_slices_dataset(df) 
  
  if (shuffle) 
    ds <- ds %>% dataset_shuffle(1000)  
  
  # fft lenght
  # by default it's the smallest power of 2 enclosing frame_length.
  fft_length <- as.integer(2^(as.integer(log(frame_length, 2)) + 1L))
  
  ds <- ds %>%
    dataset_map(function(obs) {
      
      # decoding wav files
      audio_binary <- tf$read_file(tf$reshape(obs$fname, shape = list()))
      wav <- audio_ops$decode_wav(audio_binary, desired_channels = 1)
      
      # create the spectrogram
      spectrogram <- signal$stft(
        wav$audio[,1], 
        frame_length = frame_length, 
        frame_step = frame_step, 
        pad_end = TRUE, 
        fft_length = fft_length
      )
      
      spectrogram <- tf$real(spectrogram * tf$conj(spectrogram))
      spectrogram <- tf$log(tf$abs(spectrogram) + 0.01)
      spectrogram <- tf$expand_dims(spectrogram, -1L)
      
      
      # transform the class_id into a one-hot encoded vector
      response <- tf$one_hot(obs$class_id, 29L)
      
      list(spectrogram, response)
    }) 
  
  
  ds <- ds %>% 
    dataset_repeat() %>%
    dataset_padded_batch(
      batch_size, 
      list(
        shape(as.integer(16000/frame_step), as.integer(fft_length/2 + 1), NULL), 
        shape(NULL)
      )
    )
  
  ds
}
```

Now, we can define training and validation data generators. It's worth noting that executing this won't actually compute any spectrogram and read any file. It will only define in the TensorFlow graph how it should read and pre-process data.

```{r}
set.seed(6)
id_train <- sample(nrow(df), size = 0.7*nrow(df))

ds_train <- data_generator(df[id_train,], 32)
ds_test <- data_generator(df[-id_train,], 32, shuffle = FALSE)
```

To actually get a batch from the generator we could create a TensorFlow session and ask it to run the generator. For example:

```{r}
sess <- tf$Session()
batch <- sess$run(ds_train)
str(batch)
```

Each time you run the line `batch <- sess$run(ds_train)` you should see a different batch of observations.




































